\documentclass[â€¢]{article}
%this is a comment line.

\usepackage[margin=1 in]{geometry}
\usepackage{graphicx}
\geometry{a4paper}

%These are for source code snippets in 
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\title{Detailed Documentation of Task Kernels for Xilinx Platform}
\author{SalehJG}

\begin{document}
\maketitle

\section{Intro}
This document is meant to be a detailed guide to understand the source code for each kernels and the challenges that are known for each scenario.
Most of material here has documented while Deep-Point was under development, so, they might not be updated as you would expect but they are intended to be up to date.

\subsection{Repository Information}
This document is a part of Deep-Point-FPGA project which is open-source. The Deep-Point-FGPA is a subset of Deep-Point focusing on implementing it on FPGAs.

\subsection{Programming}
\begin{itemize}
\item Xilinx Vivado 2018.2
\item Xilinx SDAccel 2018.2
\item Cent OS 7.4 X64
\end{itemize}

\pagebreak

\section{Concat2}
Concatenates two tensors of rank four into one over a dimension.
\subsection{Top Function}
\begin{lstlisting}
void task_concat(
		float* inputTn1,
	    float* inputTn2,
	    float* outputTn,

		unsigned int dimA0,
		unsigned int dimA1,
		unsigned int dimA2,
		unsigned int dimA3,

		unsigned int dimB0,
		unsigned int dimB1,
		unsigned int dimB2,
		unsigned int dimB3)
\end{lstlisting}

\subsection{Usage}
Concat Over Dim: 3
\vspace{0.5cm}
\begin{table}[htbp] % put table at (h:here, t:top, b:bottom, p:seperated page)
\caption{Usage Instances and Tensor Shapes}
\label{tab:shapes_concat}
	\begin{center}
		\begin{tabular}{|r|c|c|c|c|} 
		\hline	
		Tensor & Dim0 & Dim1 & Dim2 & Dim3\\ 
		\hline	
		InputTn1 &
			5 &
			1024 &
			1, 20 &
			3, 64, 128, 192 \\ 
		\hline
		InputTn2 &
			5 & 
			1024 & 
			1, 20 & 
			3, 64, 128 \\
		\hline
		OutputTn &
			Dim0 & 
			Dim1 & 
			Dim2 & 
			DimA3+DimB3 \\
		\hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Fixed Shape Instances}
\begin{table}[htbp] % put table at (h:here, t:top, b:bottom, p:seperated page)
\caption{Fixed Shaped Instances of Kernel Template}
\label{tab:shapes_concat}
	\begin{center}
		\begin{tabular}{|r|c|} 
		\hline	
		  & Instance 1\\ 
		\hline	
		DimA0 &
			5 \\ 
		\hline
		DimA1 & 
			1024\\
		\hline
		DimB0 & 
			5\\
		\hline
		DimB1 & 
			1024\\
		\hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Technical Details}
\begin{enumerate}
\item sadasd
\end{enumerate}


\pagebreak





\section{Sqrt}
Takes second root of each element in the input tensor of any rank.

\subsection{Top Function}
\begin{lstlisting}
void task_sqrt(
        float* inputTn,
        float* outputTn,
        unsigned long len)
\end{lstlisting}

\subsection{Usage}
Not available.

\subsection{Fixed Shape Instances}
None.

\subsection{Technical Details}
\begin{enumerate}
\item sadasd
\end{enumerate}






\pagebreak






\section{ReduceMax}
A reduction kernel with \textit{max} operator reducing on the element over the given dimension. The kernel is designed to operate on tensors of \textbf{rank three} for simplicity but it takes tensors of \textbf{rank four} in the usage cases as it is described below.
\subsection{Top Function}
\begin{lstlisting}
void task_reducemax(
        float* inputTn,
        float* outputTn,
		const unsigned int dim0,
		const unsigned int dim1,
		const unsigned int dim2,
		const int overaxis0,
		const int overaxis1,
		const int overaxis2)
\end{lstlisting}

\subsection{Usage}
Combination: FTF
\vspace{0.5cm}
\begin{table}[htbp] % put table at (h:here, t:top, b:bottom, p:seperated page)
\caption{Usage Instances and Tensor Shapes}
	\begin{center}
		\begin{tabular}{|r|c|c|c|c|} 
		\hline	
		Tensor & Dim0 & Dim1 & Dim2 & Dim3\\ 
		\hline	
		InputTn with reduction dim of 1 &
			5 &
			1024 &
			1 &
			1024 \\ 
		\hline	
		InputTn with reduction dim of 2 &
			5 &
			1024 &
			20 &
			64, 128 \\ 
		\hline
		\end{tabular}
	\end{center}
\end{table}

\begin{table}[htbp] % put table at (h:here, t:top, b:bottom, p:seperated page)
\caption{Kernel inputs for given usage instances}
	\begin{center}
		\begin{tabular}{|r|c|c|c|c|} 
		\hline	
		Reduction Dim & Arg Dim0 & Arg Dim1 & Arg Dim2 & Condition\\ 
		\hline	
		1 &
			Dim0 &
			Dim1 &
			Dim3 &
			Dim2=1 \\ 
		\hline	
		2 &
			Dim0*Dim1 &
			Dim2 &
			Dim3 &
			None \\ 
		\hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Fixed Shape Instances}
None.

\subsection{Technical Details}
\begin{enumerate}
\item sadasd
\end{enumerate}






\pagebreak






\section{Reduce Sum 4D}
A reduction kernel with summation operator, reducing elements of a four dimensional input tensor.
\subsection{Top Function}
\begin{lstlisting}
void task_reducesum4d(
        const float* inputTn,
        float* outputTn,
        const int pow_y,
        const unsigned int dim0,
        const unsigned int dim1,
        const unsigned int dim2,
        const unsigned int dim3,
        const int overaxis0,
        const int overaxis1,
        const int overaxis2,
        const int overaxis3)
\end{lstlisting}

\subsection{Usage}
Reduction Over Dim: 0,1,2 (TTTF)
\vspace{0.5cm}
\begin{table}[htbp] % put table at (h:here, t:top, b:bottom, p:seperated page)
\caption{Usage Instances and Tensor Shapes}
\label{tab:shapes_concat}
	\begin{center}
		\begin{tabular}{|r|c|c|c|c|} 
		\hline	
		Tensor & Dim0 & Dim1 & Dim2 & Dim3\\ 
		\hline	
		InputTn1 &
			5 &
			1024 &
			1, 20 &
			64, 128, 1024 \\ 
		\hline
		OutputTn &
			Dim3 & 
			 & 
			 & 
			 \\
		\hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Fixed Shape Instances}
None.

\subsection{Technical Details}
\begin{enumerate}
\item Pipelined loop with variable bound is used for moving data to and from global memory, because \emph{LOOP\_TRIPCOUNT} pragma could be used to estimate latency of the design. In contrast, it is not possible to add \emph{LOOP\_TRIPCOUNT} pragma for HLS \emph{memcpy(...)} with variable data length, meaning that latency estimation will not be available.

\item Unrolling a nested loop causes all of the sub-loops to be copied. For example if outermost loop has a trip count of four and it is completely unrolled, this means that there are four copies of any sub-loops in it.

\item A loop with burst read or write sub-loops can not be pipelined. Something like this.
\begin{lstlisting}
for(..){ // This loop can not be pipelined.
	for(..){//burst read
		..
	}
	
	for(..){//burst write
		..
	}
}
\end{lstlisting}

\item Unlike GPGPU, if-statements are favored in FPGA HLS. This is the reason of using a constant bound for-loop with a if-statement in the body of it, for checking the variable boundary.

\item For implementing \emph{pow\_y}, \emph{pow(..)} is not used, because the kernel does not require a floating point number as the power. Only the base is a floating point number. Using \emph{pow\_y} will result in higher latency.
\begin{lstlisting}
float pow_rslt = buff_tmp[i];
for(int ipwr=0;ipwr<(MAX_POW_Y_MINUS_ONE);ipwr++){
#pragma HLS UNROLL
	if(ipwr<pow_y_minus_one){
		pow_rslt = pow_rslt * pow_rslt;
	}
}
\end{lstlisting}

\item Three for-loops for \emph{dim0, dim1 and dim2} are fused together to decrease unnecessary latency of switching from one for-loop to another.

\item It might be a good idea to apply dataflow pragma in this kernel.

\end{enumerate}




\pagebreak









\section{Reduce Sum}
A reduction kernel with summation operator, reducing elements of a three dimensional input tensor.
\subsection{Top Function}
\begin{lstlisting}
void task_reducesum(
        const float * inputTn,
        float * outputTn,
        const unsigned int dim0,
        const unsigned int dim1,
        const unsigned int dim2,
        const int overaxis0,
        const int overaxis1,
        const int overaxis2)
\end{lstlisting}

\subsection{Usage}
Reduction Over Dim: 2 (FFT)
\vspace{0.5cm}
\begin{table}[htbp] % put table at (h:here, t:top, b:bottom, p:seperated page)
\caption{Usage Instances and Tensor Shapes}
\label{tab:shapes_concat}
	\begin{center}
		\begin{tabular}{|r|c|c|c|} 
		\hline	
		Tensor & Dim0 & Dim1 & Dim2\\ 
		\hline	
		InputTn1 &
			5 &
			1024 &
			3, 64 \\ 
		\hline
		OutputTn &
			Dim0 & 
			Dim1 & 
				\\
		\hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Fixed Shape Instances}
None.

\subsection{Technical Details}
\begin{enumerate}
\item This kernel also uses fused loops for \emph{dim0} and \emph{dim1}.

\item Fused loop is not pipelined, because it contains multiple sequential sub-loops.

\item Each slice of dim2 is read in burst mode and then processed by reduction sub-loop.

\item Different approaches have been tested for reduction sub-loop, details of each try and latency information is gathered in the following sub sections:

\subsubsection{Bypass with No Reduction}
Latency = 0.8 M clk

\subsubsection{Simple For-loop Reduction}
Latency = 4.3 M clk
\begin{lstlisting}
for(int i = 1 ; i< CONFIG_SLICE_SIZE;i++){
    	buff[0] += buff[i];
}
\end{lstlisting}

\subsubsection{Interleaved Parallel Reduction}
Latency = 4.0 M clk \\
Local buffer is partition with cyclic mode and factor of \emph{(CONFIG\_SLICE\_SIZE/2)}.

\begin{lstlisting}
#define CONFIG_SLICE_SIZE 64
float buff[CONFIG_SLICE_SIZE];
#pragma HLS ARRAY_PARTITION variable=buff cyclic factor=32 dim=1

//Parallel Reduction - Interleaved Addressing with Cyclic Partitioning of Local Buffer
//Compare cached slice with reduced slice(buff_rslt)
LoopReduction :for(int s=CONFIG_SLICE_SIZE/2;s>0;s>>=1){
#pragma HLS PIPELINE
#pragma HLS LOOP_FLATTEN off
#pragma HLS DEPENDENCE variable=buff array inter RAW true
	LoopIteration:for(int i=0;i<CONFIG_SLICE_SIZE/2;i++){
#pragma HLS LOOP_FLATTEN off
#pragma HLS UNROLL
#pragma HLS DEPENDENCE variable=buff array inter RAW false

		if(i<s){
			buff[i] += buff[i+s];
		}

	}
}
\end{lstlisting}

\end{enumerate}
\end{document}
